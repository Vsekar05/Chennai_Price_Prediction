# -*- coding: utf-8 -*-
"""Project_1_Chennai_Sales_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OeNuagFX4_QoWvDf2RFNMDpqzIoCGlp7

# Importing required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
# %matplotlib inline

"""# Converting the Raw data into a DataFrame"""

Data=pd.read_csv("https://raw.githubusercontent.com/Vsekar05/Datasets/main/train-chennai-sale.csv")
Data.head()

"""# Descriptive Analysis on the Data"""

Data.info()

Data.shape

Data.isna().sum()

Data.isnull().sum()

"""As we can see the Column QS_Overall has null values"""

Data.describe()

"""Changing the null values in the column Qs_overall to Mean value of that column"""

mean_value=np.mean(Data["QS_OVERALL"])
print(mean_value)

Data["QS_OVERALL"].fillna(value=mean_value, inplace=True)

Data.isna().sum()

"""The Data in the Column QS_OVERALL has been Regularized

# Exploratary Data Analysis
"""

sns.set_theme(style="darkgrid",palette="dark")

sns.distplot(Data["SALES_PRICE"])
plt.show()

"""In the Column Sales price the Data follows a Normal Distribution

The curve appears to be a Right skewed

We need to regularize the data to make it as close to Normal Distibution
"""

sns.distplot(Data["REG_FEE"],color="Green")
plt.show()

"""In the Column Reg Fee the Data follows a Normal Distribution

The curve appears to be a Right skewed

We need to regularize the data to make it as close to Normal Distibution
"""

sns.distplot(Data["COMMIS"],color="Violet")
plt.show()

"""In the Column Comms the Data follows a Normal Distribution

The curve appears to be a Right skewed

We need to regularize the data to make it as close to Normal Distibution

Since the curve for the Reg Fee and Comms and Sales price are same There appears to be a relationship between all the three columns
"""

sns.distplot(Data["INT_SQFT"],color="Yellow")
plt.show()

"""In the Column INT_SQFT the Data follows a Normal Distribution

The curve appears to be a left skewed

We need to regularize the data to make it as close to Normal Distibution
"""

sns.distplot(Data["N_BEDROOM"],color="Red")
plt.show()

"""The Column N bedrooms appears to be a orderly data"""

sns.distplot(Data["N_BATHROOM"],color="orange")
plt.show()

"""The Column N bathrooms appears to be a orderly data"""

sns.distplot(Data["N_ROOM"],color="Blue")
plt.show()

"""The Column N rooms appears to be a orderly data"""

sns.distplot(Data["QS_ROOMS"],color="Green")
plt.show()

sns.distplot(Data["QS_BEDROOM"],color="Green")
plt.show()

sns.distplot(Data["QS_BATHROOM"],color="Green")
plt.show()

"""There Appears to be a relation ship between Qs rooms , Qs beroom and Qs bathroom the curves appears to be same"""

sns.distplot(Data["QS_OVERALL"],color="Green")
plt.show()

"""The curve for the column Qs overall appears to be a normal distribution"""

fig=px.bar(Data,x="AREA",y="SALES_PRICE",color="MZZONE")
fig.show()

"""The sales price for the Area Chrompet appears to be the highest

Tha sales price fot the Area Adyr appears to be the lowest

"""

fig=px.bar(Data,x="AREA",y="REG_FEE",color="MZZONE")
fig.show()

"""The sales price for the Area KK Nagar appears to be the highest

Tha sales price fot the Area Adyr appears to be the lowest
"""

fig=px.bar(Data,x="AREA",y="COMMIS",color="MZZONE")
fig.show()

"""The sales price for the Area KK Nagar appears to be the highest

Tha sales price fot the Area Adyr appears to be the lowest

There appears to be a relationship between The columns Reg fee and Commis
"""

Data.info()

fig=px.violin(Data,x="N_BEDROOM",y="SALES_PRICE",color="MZZONE")
fig.show()

fig=px.violin(Data,x="N_ROOM",y="SALES_PRICE",color="MZZONE")
fig.show()

fig=px.violin(Data,x="N_BATHROOM",y="SALES_PRICE",color="MZZONE")
fig.show()

fig=px.scatter_matrix(Data,color="AREA",title="Scatter matrix for Chennai house price",dimensions=["SALES_PRICE","QS_OVERALL","QS_ROOMS","QS_BEDROOM","QS_BATHROOM"])
fig.show()

Data.info()

fig=px.bar(Data,x="SALE_COND",y="SALES_PRICE",color="AREA")
fig.show()

fig=px.bar(Data,x="PARK_FACIL",y="SALES_PRICE",color="AREA")
fig.show()

fig=px.bar(Data,x="BUILDTYPE",y="SALES_PRICE",color="AREA")
fig.show()

fig=px.bar(Data,x="UTILITY_AVAIL",y="SALES_PRICE",color="AREA")
fig.show()

"""From The above Graphs we can analyze how the sales price varies with all the other column available with the data set

# Finding the age of the house
"""

Data.columns

Data["DATE_SALE"]=pd.to_datetime(Data["DATE_SALE"])

Data["DATE_BUILD"]=pd.to_datetime(Data["DATE_BUILD"])

Data["YEAR_BUILD"]=Data["DATE_BUILD"].dt.year

Data["YEAR_SOLD"]=Data["DATE_SALE"].dt.year

Data.head()

Data["AGE_OF_HOUSE"]=Data["YEAR_SOLD"]-Data["YEAR_BUILD"]

Data.head()

import matplotlib.pyplot as plt
import seaborn as sns
sns.distplot(Data["AGE_OF_HOUSE"],color="Green")
plt.show()

"""The above graph shows that most of the house in the data set are in the range of 25 to 30 years old"""

import plotly.express as px
fig=px.bar(Data,x="AREA",y="SALES_PRICE",color="AGE_OF_HOUSE")
fig.show()

"""The above graph shows how the price varies with the age of the house

# Encoding the Data
"""

Data1=pd.read_csv("https://raw.githubusercontent.com/Vsekar05/Datasets/main/train-chennai-sale.csv")
Data1.head()

Data.isna().sum()

"""Stemming missspelled words"""

Data["AREA"].unique()

Data["AREA"].replace("Ann Nagar","Anna Nagar")

Data["AREA"]=Data["AREA"].replace("Ann Nagar","Anna Nagar")

Data["AREA"].unique()

Data["AREA"]=Data["AREA"].replace("Ana Nagar","Anna Nagar")

Data["AREA"]=Data["AREA"].replace("Karapakam","Karapakkam")

Data["AREA"]=Data["AREA"].replace("TNagar","T Nagar")

Data["AREA"]=Data["AREA"].replace("Adyr","Adyar")

Data["AREA"]=Data["AREA"].replace("Velchery","Velachery")

Data["AREA"]=Data["AREA"].replace("Chrompt","Chrompet")

Data["AREA"]=Data["AREA"].replace("Chrmpet","Chrompet")

Data["AREA"]=Data["AREA"].replace("Chormpet","Chrompet")

Data["AREA"]=Data["AREA"].replace("KKNagar","KK Nagar")

Data["AREA"].unique()

from sklearn import preprocessing
le=preprocessing.LabelEncoder()
Data["AREA"]=le.fit_transform(Data["AREA"])

Data["AREA"].unique()

Data.head()

Data["PARK_FACIL"]=Data["PARK_FACIL"]

Data.head()

Data["PARK_FACIL"].unique()

Data["PARK_FACIL"]=Data["PARK_FACIL"].replace("Noo","No")

Data["PARK_FACIL"].unique()

Data["PARK_FACIL"]=le.fit_transform(Data["PARK_FACIL"])

Data["PARK_FACIL"].unique()

Data.info()

Data["SALE_COND"].unique()

Data["SALE_COND"]=Data["SALE_COND"].replace("Ab Normal","AbNormal")

Data["SALE_COND"]=Data["SALE_COND"].replace("Partiall","Partial")

Data["SALE_COND"]=Data["SALE_COND"].replace("PartiaLl","Partial")

Data["SALE_COND"]=Data["SALE_COND"].replace("Adj Land","AdjLand")

Data["SALE_COND"].unique()

Data["SALE_COND"]=le.fit_transform(Data["SALE_COND"])

Data["BUILDTYPE"].unique()

Data["BUILDTYPE"]=Data["BUILDTYPE"].replace("Other","Others")

Data["BUILDTYPE"]=Data["BUILDTYPE"].replace("Comercial","Commercial")

Data["BUILDTYPE"].unique()

Data["BUILDTYPE"]=le.fit_transform(Data["BUILDTYPE"])

Data["UTILITY_AVAIL"].unique()

Data["UTILITY_AVAIL"]=Data["UTILITY_AVAIL"].replace("All Pub","AllPub")

Data["UTILITY_AVAIL"]=Data["UTILITY_AVAIL"].replace("NoSewr ","NoSeWa")

Data["UTILITY_AVAIL"].unique()

Data["UTILITY_AVAIL"]=le.fit_transform(Data["UTILITY_AVAIL"])

Data["UTILITY_AVAIL"].unique()

Data["STREET"].unique()

Data["STREET"]=Data["STREET"].replace("Pavd","Paved")

Data["STREET"]=Data["STREET"].replace("NoAccess","No Access")

Data["STREET"].unique()

Data["STREET"]=le.fit_transform(Data["STREET"])

Data["MZZONE"].unique()

Data["MZZONE"]=le.fit_transform(Data["MZZONE"])

Data.info()

Data.isna().sum()

mean_value=np.mean(Data["N_BEDROOM"])
print(mean_value)

Data["N_BEDROOM"].fillna(value=mean_value, inplace=True)

mean_value=np.mean(Data["N_BATHROOM"])
print(mean_value)

Data["N_BATHROOM"].fillna(value=mean_value, inplace=True)

Data.isna().sum()

"""# Machine Learning

# Linear Regression
"""

New_Data=Data.copy()

New_Data.head()

New_Data.drop(['PRT_ID','DATE_SALE','REG_FEE','COMMIS','DATE_BUILD'],axis=1,inplace=True)

New_Data.drop(['YEAR_BUILD','YEAR_SOLD'],axis=1,inplace=True)

New_Data.head()

New_Data.to_csv("Final_Data.csv")

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X=New_Data.drop(columns=["SALES_PRICE"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

model = LinearRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

import numpy as np
from sklearn import metrics
print("Mean Absolute Error:",metrics.mean_absolute_error(y_test,y_pred))
print("Mean Squared Error:",metrics.mean_squared_error(y_test,y_pred))
print("Root Mean Squared Error:",np.sqrt(metrics.mean_squared_error(y_test,y_pred)))

score = model.score(X_train,y_train)
print(score*100,"%")

plt.plot(X,y,"o")
plt.plot(X,model.predict(X))
plt.show()

scores_1={
    'Linear Regression':{
        'Train': model.score(X_train,y_train),
        'Test': model.score(X_test,y_test)
    },
}

New_Data.info()

model.predict([['4','2000','10','2','2','5','1','0','1','1','2','5','10','10','10','50','10']])

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X=New_Data.drop(columns=["SALES_PRICE"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize']=(10,10)
sns.heatmap(New_Data.corr(),annot=True)

model = LogisticRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

import numpy as np
from sklearn import metrics
print("Mean Absolute Error:",metrics.mean_absolute_error(y_test,y_pred))
print("Mean Squared Error:",metrics.mean_squared_error(y_test,y_pred))
print("Root Mean Squared Error:",np.sqrt(metrics.mean_squared_error(y_test,y_pred)))

score = model.score(X_train,y_train)
print(score*100,"%")

plt.plot(X,y,"o")
plt.plot(X,model.predict(X))

scores={
    'Logistic Regression':{
        'Train': model.score(X_train,y_train),
        'Test': model.score(X_test,y_test)
    },
}

"""# Nearest Neighbours"""

from sklearn.model_selection import train_test_split
X=New_Data.drop(columns=["SALES_PRICE"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

from sklearn.neighbors import KNeighborsClassifier

k=4
model = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
model

yhat = model.predict(X_test)

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, model.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))

K = 10
mean_acc = np.zeros((K-1))
std_acc = np.zeros((K-1))
ConfustionMx = [];
for n in range(1,K):
  
   #Train Model and Predict 
   neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
   yhat=neigh.predict(X_test)
   mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
 
  
   std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])
 
mean_acc

import matplotlib.pyplot as plt
plt.plot(range(1,K),mean_acc,'g')
plt.fill_between(range(1,K),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1)

scores={
    'K Nearest Neighbours':{
        'Train': metrics.accuracy_score(y_train, model.predict(X_train)),
        'Test': metrics.accuracy_score(y_test, yhat)
    },
}

""" # Ensemble Learning"""

from sklearn.model_selection import train_test_split


X=New_Data.drop(columns=["SALES_PRICE"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.ensemble import BaggingRegressor
from sklearn.linear_model import LinearRegression

model = LinearRegression()
bagging_reg_lin= BaggingRegressor(base_estimator=model, n_estimators=1500, random_state=42)
bagging_reg_lin.fit(X_train, y_train)
bagging_reg_lin.score(X_test,y_test)

scores = {
    'Bagging Regressor linear': {
        'Train': bagging_reg_lin.score(X_train,y_train),
        'Test': bagging_reg_lin.score(X_test,y_test),
    },
}

from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor()
bagging_reg_tree = BaggingRegressor(base_estimator=model, n_estimators=1500, random_state=42)
bagging_reg_tree.fit(X_train, y_train)
bagging_reg_tree.score(X_test,y_test)

scores = {
    'Bagging Regressor Decsion Tree': {
        'Train': bagging_reg_tree.score(X_train,y_train),
        'Test': bagging_reg_tree.score(X_test,y_test),
    },
}

from sklearn.ensemble import AdaBoostRegressor

ada_boost_reg = AdaBoostRegressor(n_estimators=30)
ada_boost_reg.fit(X_train, y_train)
ada_boost_reg.score(X_test,y_test)

scores['AdaBoost'] = {
        'Train': ada_boost_reg.score(X_train,y_train),
        'Test': ada_boost_reg.score(X_test,y_test),
    }

from sklearn.ensemble import GradientBoostingRegressor

grad_boost_reg = GradientBoostingRegressor(n_estimators=10, random_state=42)
grad_boost_reg.fit(X_train, y_train)
grad_boost_reg.score(X_test, y_test)

scores['Gradient Boosting'] = {
        'Train': grad_boost_reg.score(X_train,y_train),
        'Test': grad_boost_reg.score(X_test,y_test),
    }

from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor


estimators = []
lin_reg = LinearRegression()
estimators.append(('Logistic', lin_reg))

tree = DecisionTreeRegressor()
estimators.append(('Tree', tree))

ada_reg=AdaBoostRegressor()
estimators.append(('adaboost', ada_reg))

grad_reg=GradientBoostingRegressor()
estimators.append(('gradboost', grad_reg))

voting = VotingRegressor(estimators=estimators)
voting.fit(X_train, y_train)

voting.score(X_test,y_test)

scores['Voting'] = {
        'Train': voting.score(X_train,y_train),
        'Test': voting.score(X_test,y_test)
    }

scores_df=pd.DataFrame(scores)

scores_df

scores_df.plot(kind='barh', figsize=(15, 8))

"""# Selecting Features based on the best ml model"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X=New_Data.drop(columns=["SALES_PRICE"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

rfr = RandomForestRegressor(n_estimators=100, random_state=0)
rfr.fit(X_train, y_train)
y_pred = rfr.predict(X_test)
rfr.score(X_test,y_test)

reg = RandomForestRegressor(n_estimators=5, random_state=0)
reg.fit(X_train, y_train)
feature_scores = pd.Series(reg.feature_importances_, index=X_train.columns).sort_values(ascending=False)
feature_scores

sns.barplot(x=feature_scores, y=feature_scores.index)
plt.xlabel('Feature Importance Score')

plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

New_Data.info()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X=New_Data.drop(columns=["SALES_PRICE","UTILITY_AVAIL","QS_BEDROOM","SALE_COND","QS_ROOMS","QS_BATHROOM","DIST_MAINROAD","QS_OVERALL"])
y=New_Data["SALES_PRICE"]
print(X.shape)
print(y.shape)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

rfr = RandomForestRegressor(n_estimators=100, random_state=0)
rfr.fit(X_train, y_train)
y_pred = rfr.predict(X_test)
rfr.score(X_test,y_test)

print("The maximum score that i got for predicting the Sales price in the given data after removing the unwanter features is",rfr.score(X_test,y_test)*100,"%")

import pickle
filename="rfr.pkl"
pickle.dump(rfr,open(filename,'wb'))

"""# Predicting the Sales Price """

X.columns

X["AREA"].unique()

rfr.predict([["4","2000","3","3","5","0","2","1","2","20"]])

"""# The price range for the predicted value(Sales Price)

Since the predicted score for the Data is 98.41% . The Overall price can in a range of +/- 1.79% of the price
"""

l=rfr.predict([["4","2000","3","3","5","0","2","1","2","20"]])

c=l-(0.0159*l)
d=l+(0.0159*l)
print('The Lowest price for the given data:',int(c))
print('The Highest price for the given data:',int(d))
print('The price range for',int(l),'is',int(c),'-',int(d))

"""# Overall Price Prediction"""

Data.head()

New_Data_2=Data.copy()

New_Data_2.head()

New_Data_2["OVERALL_PRICE"]=New_Data_2["REG_FEE"]+New_Data_2["COMMIS"]+New_Data_2["SALES_PRICE"]

New_Data_2.head()

New_Data_2.drop(['PRT_ID','DATE_SALE','REG_FEE','COMMIS','DATE_BUILD','YEAR_SOLD'],axis=1,inplace=True)

New_Data_2.drop(['YEAR_BUILD'],axis=1,inplace=True)

New_Data_2.drop(['SALES_PRICE'],axis=1,inplace=True)

New_Data_2.head()

New_Data_2.to_csv("Final_Data_2.csv")

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X=New_Data_2.drop(columns=["OVERALL_PRICE"])
y=New_Data_2["OVERALL_PRICE"]
print(X.shape)
print(y.shape)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

rfr = RandomForestRegressor(n_estimators=100, random_state=0)
rfr.fit(X_train, y_train)
y_pred = rfr.predict(X_test)
rfr.score(X_test,y_test)

reg = RandomForestRegressor(n_estimators=100, random_state=0)
reg.fit(X_train, y_train)
feature_scores = pd.Series(reg.feature_importances_, index=X_train.columns).sort_values(ascending=False)
feature_scores

sns.barplot(x=feature_scores, y=feature_scores.index)
plt.xlabel('Feature Importance Score')

plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X=New_Data_2.drop(columns=["OVERALL_PRICE","UTILITY_AVAIL","QS_BEDROOM","SALE_COND","QS_ROOMS","QS_BATHROOM","DIST_MAINROAD","QS_OVERALL"])
y=New_Data_2["OVERALL_PRICE"]
print(X.shape)
print(y.shape)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)

rfr_2 = RandomForestRegressor(n_estimators=100, random_state=0)
rfr_2.fit(X_train, y_train)
y_pred = rfr_2.predict(X_test)
rfr_2.score(X_test,y_test)

print("The maximum score that i got for predicting the Overall price in the given data after removing the unwanter features is",rfr_2.score(X_test,y_test)*100,"%")

import pickle
filename="rfr_2.pkl"
pickle.dump(rfr_2,open(filename,'wb'))

"""# The price range for the predicted value(Overall Price)

Since the predicted score for the Data is 98.34% . The Overall price can in a range of +/- 1.66% of the price
"""

rfr_2.predict([["4","2000","3","3","5","0","2","1","2","20"]])

k=rfr_2.predict([["4","2000","3","3","5","0","2","1","2","20"]])

a=k-(0.0166*k)
b=k+(0.0166*k)
print('The Lowest price for the given data:',int(a))
print('The Highest price for the given data:',int(b))
print('The price range for',int(k),'is',int(a),'-',int(b))